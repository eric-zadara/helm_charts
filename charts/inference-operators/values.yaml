# Inference operators umbrella chart

# cert-manager - TLS certificate management operator
cert-manager:
  # -- Enable cert-manager deployment
  enabled: true
  # CRDs are installed by inference-crds chart (must be installed first)
  crds:
    enabled: false

# CloudNativePG operator - PostgreSQL operator for Kubernetes
cloudnative-pg:
  # -- Enable CNPG operator deployment
  enabled: true

# KServe controller - reconciles InferenceService, ClusterServingRuntime resources
kserve:
  # -- Enable KServe controller deployment
  enabled: true
  kserve:
    controller:
      # Use Knative deployment mode (v0.16.0 renamed "Serverless" to "Knative")
      deploymentMode: "Knative"
      resources:
        limits:
          cpu: 100m
          memory: 300Mi
        requests:
          cpu: 100m
          memory: 300Mi
    # Disable ALL built-in ClusterServingRuntimes
    # This project provides its own runtimes via model-serving chart (vLLM, llama.cpp, Ollama)
    servingruntime:
      tensorflow:
        disabled: true
      mlserver:
        disabled: true
      sklearnserver:
        disabled: true
      xgbserver:
        disabled: true
      huggingfaceserver:
        disabled: true
      tritonserver:
        disabled: true
      pmmlserver:
        disabled: true
      paddleserver:
        disabled: true
      lgbserver:
        disabled: true
      torchserve:
        disabled: true
    # Disable features not needed for this project
    localmodel:
      enabled: false

# Knative Operator - manages Knative Serving/Eventing via CRs
knative-operator:
  # -- Enable Knative Operator deployment
  enabled: true

# KnativeServing CR - reconciled by knative-operator above.
# Placed here (alongside operator + KServe) so Knative Serving is ready
# before inference-stack creates InferenceServices. KServe caches a terminal
# error if Knative isn't available at first reconciliation.
knativeServing:
  # -- Enable KnativeServing CR creation
  enabled: true
  namespace: "knative-serving"
  spec:
    version: "1.21"
    ingress:
      kourier:
        enabled: true
        # ClusterIP only - Envoy Gateway handles external traffic
        service-type: ClusterIP
    config:
      network:
        ingress-class: "kourier.ingress.networking.knative.dev"
      deployment:
        # Extended progress deadline for GPU model loading (default 600s is too short)
        progress-deadline: "1200s"
      autoscaler:
        enable-scale-to-zero: "true"
        scale-to-zero-grace-period: "30s"
        # Extended scale-down delay to avoid thrashing on bursty LLM inference traffic
        scale-down-delay: "300s"
      features:
        # Allow pods to set terminationGracePeriodSeconds for graceful model unloading
        kubernetes.podspec-terminationGracePeriodSeconds: "Enabled"
        # Allow tolerations for GPU node scheduling
        kubernetes.podspec-tolerations: "Enabled"
        # Allow nodeSelector for GPU type selection
        kubernetes.podspec-nodeselector: "Enabled"
        # Allow affinity rules for advanced scheduling
        kubernetes.podspec-affinity: "Enabled"

# Envoy Gateway - Gateway API controller
envoy-gateway:
  # -- Enable Envoy Gateway deployment
  enabled: true

# NVIDIA GPU Operator - manages GPU device plugin, runtime, monitoring
gpu-operator:
  # -- Enable NVIDIA GPU Operator deployment
  enabled: true
  # Skip driver installation (pre-installed on GPU VMs)
  driver:
    enabled: false

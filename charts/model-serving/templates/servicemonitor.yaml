{{- if .Values.serviceMonitor.enabled }}
# ServiceMonitor for vLLM metrics exposed by KServe InferenceService pods
# vLLM exposes metrics on port 8080 at /metrics path
# Key metrics: vllm:num_requests_running, vllm:gpu_cache_usage_perc, vllm:time_to_first_token_seconds
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: {{ include "model-serving.fullname" . }}-vllm
  labels:
    {{- include "model-serving.labels" . | nindent 4 }}
    app.kubernetes.io/component: vllm-metrics
spec:
  selector:
    matchLabels:
      # Select KServe predictor services for any InferenceService
      # This is a broad selector - customize with additional labels if needed
      component: predictor
  endpoints:
    - port: http
      path: /metrics
      interval: {{ .Values.serviceMonitor.interval }}
      scrapeTimeout: {{ .Values.serviceMonitor.scrapeTimeout }}
      {{- if .Values.serviceMonitor.relabelings }}
      relabelings:
        {{- toYaml .Values.serviceMonitor.relabelings | nindent 8 }}
      {{- end }}
  namespaceSelector:
    {{- if .Values.serviceMonitor.namespaceSelector }}
    {{- toYaml .Values.serviceMonitor.namespaceSelector | nindent 4 }}
    {{- else }}
    matchNames:
      - {{ .Release.Namespace }}
    {{- end }}
{{- end }}

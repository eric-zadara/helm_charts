# charts/litellm-proxy/values.yaml
#
# ZERO-CONFIG DEFAULTS:
# This chart works out of the box with `helm install litellm-proxy`.
# Internal PostgreSQL (CNPG) and Valkey are created automatically.
#
# POWER USER OVERRIDES:
# Disable internal resources and provide external connection details:
#   database.internal.enabled: false + database.external.secretName
#   cache.internal.enabled: false + cache.external.secretName

# -- Docker image configuration
# @section -- Image Configuration
image:
  # -- Docker image repository (database variant includes Prisma support)
  # Use docker.litellm.ai registry as recommended by LiteLLM docs
  repository: "ghcr.io/berriai/litellm-database"
  # -- Image tag (use main-stable or pin to specific version like main-v1.81.0)
  # See: https://docs.litellm.ai/docs/proxy/deploy
  tag: "main-v1.81.3-stable"
  # -- Image pull policy
  pullPolicy: IfNotPresent

# -- Image pull secrets for private registries
# @section -- Image Configuration
imagePullSecrets: []

# -- Number of LiteLLM proxy replicas
# @section -- Image Configuration
replicaCount: 2

# -- Override chart name
# @section -- Image Configuration
nameOverride: ""

# -- Override full release name
# @section -- Image Configuration
fullnameOverride: ""

# -- Master key configuration for admin API authentication
# @section -- Authentication
masterKey:
  # -- Create a Kubernetes Secret from the value below (set false to use existing)
  create: true
  # -- Name of an existing secret containing the master key
  existingSecret: ""
  # -- Key within the existing secret
  existingSecretKey: "master-key"
  # -- Master key value (auto-generated if empty and create=true)
  value: ""

# -- Salt key for encrypting API credentials stored in PostgreSQL.
# WARNING: Never change after initial deployment with data -- existing encrypted
# data becomes unreadable if the salt key changes.
# @section -- Authentication
saltKey:
  # -- Create a Kubernetes Secret from the value below (set false to use existing)
  create: true
  # -- Name of an existing secret containing the salt key
  existingSecret: ""
  # -- Key within the existing secret
  existingSecretKey: "salt-key"
  # -- Salt key value (auto-generated if empty and create=true)
  value: ""

# -- Database configuration (PostgreSQL)
# @section -- Database Configuration
database:
  # -- Internal CNPG PostgreSQL cluster (enabled by default)
  # Creates a highly-available PostgreSQL cluster with PgBouncer pooling
  internal:
    # -- Enable internal CNPG cluster deployment
    enabled: true
    # -- Number of PostgreSQL instances (1 for dev, 3 for HA)
    instances: 1
    # -- PostgreSQL image
    imageName: "ghcr.io/cloudnative-pg/postgresql:17.4"
    # -- Storage size for PostgreSQL data
    storageSize: "10Gi"
    # -- Storage class (leave empty for default)
    storageClass: ""
    # -- Database name to create
    database: "litellm"
    # -- Owner username
    owner: "litellm"
    # -- PostgreSQL parameters
    parameters:
      max_connections: "200"
      shared_buffers: "256MB"
    # -- Resource requests/limits
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "1Gi"
        cpu: "1000m"
    # -- PgBouncer pooler configuration
    pooler:
      # -- Enable PgBouncer connection pooling
      enabled: true
      # -- Number of pooler instances
      instances: 2
      # -- Pool mode (transaction recommended)
      poolMode: "transaction"
      # -- Default connections per pool
      defaultPoolSize: 50
      # -- Maximum client connections
      maxClientConn: 1000

  # -- External database connection (used when internal.enabled=false)
  external:
    # -- PostgreSQL host
    host: ""
    # -- PostgreSQL port
    port: 5432
    # -- Database name
    name: "litellm"
    # -- Database user
    user: "litellm"
    # -- Secret containing the password (required when internal.enabled=false)
    # Secret must have key specified in passwordSecretKey
    secretName: ""
    # -- Key within the password secret
    passwordSecretKey: "password"

  # -- Connection pool limit per worker process
  # Formula: PgBouncer_max_client_conn / (num_workers x num_pods)
  connectionPoolLimit: 10
  # -- Connection timeout in seconds
  connectionTimeout: 60

# -- Cache configuration (Redis/Valkey)
# @section -- Cache Configuration
cache:
  # -- Enable caching and distributed rate limiting
  enabled: true

  # -- Internal Valkey deployment (enabled by default)
  # Creates a Valkey instance for caching and rate limiting
  internal:
    # -- Enable internal Valkey deployment
    enabled: true
    # -- Number of Valkey replicas
    replicas: 1
    # -- Valkey image
    image: "docker.io/valkey/valkey:8-alpine"
    # -- Storage persistence
    persistence:
      enabled: false
      size: "1Gi"
      storageClass: ""
    # -- Resource requests/limits
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "500m"
    # -- Password for Valkey (auto-generated if empty)
    password: ""

  # -- External cache connection (used when internal.enabled=false)
  external:
    # -- Redis/Valkey host
    host: ""
    # -- Redis/Valkey port
    port: 6379
    # -- Secret containing the password (required when internal.enabled=false)
    secretName: ""
    # -- Key within the password secret
    passwordSecretKey: "password"
    # -- Enable Sentinel mode for HA
    sentinel:
      enabled: false
      # -- Sentinel port
      port: 26379
      # -- Sentinel master name
      serviceName: "mymaster"

# -- LiteLLM proxy configuration (generates litellm_config.yaml)
# @section -- LiteLLM Configuration
litellm:
  # -- Model list (array of model definitions routed to KServe endpoints).
  # Each entry creates a model accessible via /v1/chat/completions.
  # See README for routing pattern examples.
  #
  # Example: Route through Internal Gateway for KV-cache aware routing
  # modelList:
  #   - modelName: gpt-4
  #     litellmParams:
  #       model: hosted_vllm/meta-llama/Llama-3.3-70B-Instruct
  #       # Use Internal Gateway for KV-cache aware routing (recommended):
  #       apiBase: http://inference-gateway.llm-platform.svc.cluster.local
  #       # Or use direct predictor URL (simpler, no KV-cache routing):
  #       # apiBase: http://llama-70b-predictor.llm-platform.svc.cluster.local
  modelList: []

  # -- General settings (merged into config.yaml general_settings)
  generalSettings:
    # -- Batch spend writes every N seconds (reduces DB load)
    proxy_batch_write_at: 60
    # -- Allow requests if DB is temporarily unavailable
    allow_requests_on_db_unavailable: true
    # -- Request timeout in seconds
    request_timeout: 600
    # -- Disable verbose error logs in DB (recommended for production)
    disable_error_logs: true

  # -- LiteLLM settings (merged into config.yaml litellm_settings)
  litellmSettings:
    # -- Enable response caching
    cache: true
    # -- Cache parameters (infrastructure wiring added automatically from cache.* values)
    cache_params:
      type: "redis"
      # -- Cache TTL in seconds
      ttl: 600
      # -- Cache key namespace
      namespace: "litellm.caching"
      # -- Cache mode: "default_off" requires per-request opt-in, "default_on" caches all
      mode: "default_off"
    # -- Enable JSON structured logging
    json_logs: true
    # -- Disable verbose output (recommended for production)
    set_verbose: false
    # -- Prometheus callbacks for metrics export
    callbacks:
      - "prometheus"
    # -- Fallback chains (model group name -> fallback model groups)
    fallbacks: []
    # -- Turn off message content logging (privacy)
    turn_off_message_logging: false
    # -- Number of retries on failure
    num_retries: 2

  # -- Router settings (merged into config.yaml router_settings)
  routerSettings:
    # -- Routing strategy
    routing_strategy: "simple-shuffle"
    # -- Model group aliases (supports hidden models)
    model_group_alias: {}
    # -- Enable pre-call checks (context window, model availability)
    enable_pre_call_checks: true

# -- Request logging configuration
# @section -- Logging
logging:
  # -- Store prompts/responses in spend_logs (false for privacy)
  storePrompts: false
  # -- Disable spend logs entirely (saves DB space, lose UI usage view)
  disableSpendLogs: false
  # -- Spend log retention period (e.g., "30d", "90d")
  retentionPeriod: "30d"
  # -- Retention cleanup interval
  retentionInterval: "1d"

# -- Service configuration
# @section -- Service Configuration
service:
  # -- Service type
  type: ClusterIP
  # -- Service port (LiteLLM default)
  port: 4000

# -- HTTPRoute configuration (Envoy Gateway integration)
# @section -- HTTPRoute Configuration
httpRoute:
  # -- Enable HTTPRoute creation
  enabled: true
  # -- Gateway reference
  gateway:
    # -- Gateway name (defaults to networking-layer gateway)
    name: ""
    # -- Gateway namespace (defaults to release namespace)
    namespace: ""
  # -- Route hostname (optional, for host-based routing)
  hostname: ""
  # -- Route path prefix
  pathPrefix: "/"

# -- Health check configuration
# @section -- Health Checks
healthCheck:
  # -- Use separate health check app/port (recommended for production).
  # Prevents health check timeouts under heavy load.
  separateApp: true
  # -- Separate health check port
  separatePort: 8001
  # -- Startup probe (allows time for LiteLLM initialization)
  startup:
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 30
  # -- Liveness probe
  liveness:
    periodSeconds: 15
    timeoutSeconds: 5
    failureThreshold: 3
  # -- Readiness probe
  readiness:
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

# -- Migration job configuration (Prisma schema migration as Helm post-install/post-upgrade hook)
# @section -- Migration Job
migrationJob:
  # -- Enable Prisma migration job
  enabled: true
  # -- Job retry limit
  backoffLimit: 4
  # -- TTL after completion (seconds)
  ttlSecondsAfterFinished: 120

# -- Init container configuration for waiting on dependencies
# @section -- Init Containers
initContainers:
  # -- Wait for database to be reachable before starting (recommended for production)
  waitForDatabase: true
  # -- Wait for cache to be reachable before starting (recommended for production)
  waitForCache: true

# -- Resource requirements
# @section -- Resources and Scaling
resources:
  requests:
    cpu: "500m"
    memory: "512Mi"
  limits:
    cpu: "2000m"
    memory: "2Gi"

# -- HPA configuration
# @section -- Resources and Scaling
autoscaling:
  # -- Enable HPA
  enabled: false
  # -- Minimum replicas
  minReplicas: 2
  # -- Maximum replicas
  maxReplicas: 10
  # -- Target CPU utilization percentage
  targetCPUUtilizationPercentage: 80
  # -- Target memory utilization percentage (optional, empty to disable)
  targetMemoryUtilizationPercentage: ""

# -- PodDisruptionBudget configuration
# @section -- Resources and Scaling
podDisruptionBudget:
  # -- Enable PDB
  enabled: false
  # -- Minimum available pods during disruptions
  minAvailable: 1

# -- Termination grace period (allows in-flight requests to complete)
# @section -- Resources and Scaling
terminationGracePeriodSeconds: 90

# -- Prometheus ServiceMonitor
# @section -- Observability
serviceMonitor:
  # -- Enable ServiceMonitor creation
  enabled: false
  # -- Scrape interval
  interval: "15s"
  # -- Scrape timeout
  scrapeTimeout: "10s"

# -- Grafana dashboard configuration
# @section -- Observability
dashboards:
  # -- Enable dashboard ConfigMap creation (requires Grafana sidecar)
  enabled: false
  # -- Grafana folder annotation for dashboard organization
  folderAnnotation: "LLM Platform"

# -- Node selector constraints
# @section -- Pod Configuration
nodeSelector: {}

# -- Tolerations for pod scheduling
# @section -- Pod Configuration
tolerations: []

# -- Affinity rules for pod scheduling
# @section -- Pod Configuration
affinity: {}

# -- Additional pod annotations
# @section -- Pod Configuration
podAnnotations: {}

# -- Additional pod labels
# @section -- Pod Configuration
podLabels: {}

# -- Extra environment variables as key-value pairs
# @section -- Pod Configuration
extraEnv: {}

# -- Extra environment variable sources (secretRef, configMapRef)
# @section -- Pod Configuration
extraEnvFrom: []

# =============================================================================
# SUBCHART OVERRIDES
# These are passed to the dependency subcharts when internal resources are enabled
# =============================================================================

# CNPG cluster subchart configuration (only used when database.internal.enabled=true)
# Override any value from https://github.com/cloudnative-pg/charts/tree/main/charts/cluster
postgresql:
  mode: standalone
  type: postgresql

  cluster:
    instances: 1
    imageName: "ghcr.io/cloudnative-pg/postgresql:17.4"

    # Bootstrap configuration - creates database and owner
    initdb:
      database: "litellm"
      owner: "litellm"

    postgresql:
      parameters:
        max_connections: "200"
        shared_buffers: "256MB"

    storage:
      size: "10Gi"

    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "1Gi"
        cpu: "1000m"

  poolers:
    - name: rw
      type: rw
      instances: 2
      poolMode: transaction
      parameters:
        default_pool_size: "50"
        max_client_conn: "1000"

# Valkey subchart configuration (only used when cache.internal.enabled=true)
# Override any value from https://github.com/bitnami/charts/tree/main/bitnami/valkey
valkey:
  architecture: standalone

  auth:
    enabled: true
    password: ""

  master:
    persistence:
      enabled: false

    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "500m"

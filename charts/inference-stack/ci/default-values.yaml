# CI test values for inference-stack
# Minimal overlay on production defaults -- only override what differs for CI.
# Prerequisites: inference-operators (CRDs + CNPG), inference-infrastructure (networking + DB)

# CI uses "inference-infrastructure" as the infrastructure release name
infrastructureReleaseName: "inference-infrastructure"
infrastructureNamespace: "default"

model-serving:
  # ClusterServingRuntimes - ENABLED (KServe controller deployed by inference-operators)
  # These reconcile immediately without starting any pods.
  # Production defaults (enabled: true) are used -- no override needed.

  # InferenceServices - DISABLED (require actual model serving pods, GPUs, storage)
  # Even with KServe controller running, these create pods that need real infrastructure.
  # This is a permanent CI limitation, not a TODO.
  inferenceService:
    enabled: false
  llamacppInferenceService:
    enabled: false
  ollamaInferenceService:
    enabled: false

  # InferencePool - enabled (inference-extension-crds available)
  inferencePool:
    enabled: true

  # PriorityClasses - standard K8s resources
  priorityClasses:
    enabled: true

inference-gateway:
  # EPP - single replica and reduced resources for CI
  epp:
    replicas: 1
    poolName: "ci-test-pool"
    poolNamespace: "default"
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi

  # Test InferencePool for EPP to route to
  testPool:
    enabled: true
    targetPort: 8000
    selector:
      app: test-model-server
    failureMode: "FailOpen"

  # CI mock model server for EPP endpoints
  ci:
    modelServer:
      enabled: true

# CI test values for inference-stack
# Minimal overlay on production defaults -- only override what differs for CI.
# Prerequisites: inference-operators (CRDs + CNPG), inference-infrastructure (networking + DB)

# CI uses "inference-infrastructure" as the infrastructure release name
infrastructureReleaseName: "inference-infrastructure"
infrastructureNamespace: "default"

model-serving:
  # ClusterServingRuntimes - DISABLED (requires KServe CONTROLLER, not just CRDs)
  # KServe controller is not included in inference-operators.
  # TODO(operators): Add KServe controller to inference-operators chart,
  #   then remove these overrides to test with production defaults.
  servingRuntime:
    enabled: false
  llamacppRuntime:
    enabled: false
  ollamaRuntime:
    enabled: false

  # InferenceServices - DISABLED (requires KServe CONTROLLER)
  inferenceService:
    enabled: false
  llamacppInferenceService:
    enabled: false
  ollamaInferenceService:
    enabled: false

  # InferencePool - enabled (inference-extension-crds available)
  inferencePool:
    enabled: true

  # PriorityClasses - standard K8s resources
  priorityClasses:
    enabled: true

inference-gateway:
  # EPP - single replica and reduced resources for CI
  epp:
    replicas: 1
    poolName: "ci-test-pool"
    poolNamespace: "default"
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi

  # Test InferencePool for EPP to route to
  testPool:
    enabled: true
    targetPort: 8000
    selector:
      app: test-model-server
    failureMode: "FailOpen"

  # CI mock model server for EPP endpoints
  ci:
    modelServer:
      enabled: true

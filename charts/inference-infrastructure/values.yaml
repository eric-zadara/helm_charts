# Inference infrastructure umbrella chart

# Gateway API resources - GatewayClass, EnvoyProxy config, and public-facing Gateway
gateway:
  # -- Enable Gateway API resource creation
  enabled: true
  # GatewayClass name (referenced by HTTPRoutes across the platform)
  className: "llm-gateway"
  # Service configuration for the Envoy proxy
  service:
    # -- Service type (LoadBalancer, ClusterIP, NodePort)
    type: LoadBalancer
    # -- Annotations for the envoy proxy Service
    # AWS LBC defaults to internal; explicitly set internet-facing for public access
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
  listeners:
    http:
      # -- Enable HTTP listener
      enabled: true
      port: 80
    https:
      # -- Enable HTTPS listener
      enabled: false
      port: 443
      # -- Hostname for TLS (e.g. "llm.example.com"). Omit for wildcard.
      hostname: ""
      # -- Name of the TLS Secret (auto-created by cert-manager if enabled)
      certificateRef: "llm-gateway-tls"
      # cert-manager integration
      certManager:
        # -- Enable cert-manager annotation on Gateway for automatic certificate provisioning
        enabled: true
        # -- ClusterIssuer name (must exist in cluster)
        clusterIssuer: "letsencrypt-prod"

# LiteLLM API proxy (bundles internal PostgreSQL + Valkey)
litellm-proxy:
  # -- Enable LiteLLM proxy deployment
  enabled: true
  # Use internal database and cache (self-contained)
  database:
    internal:
      enabled: true
  cache:
    internal:
      enabled: true
  # HTTPRoute - wire to the umbrella's Envoy Gateway
  httpRoute:
    enabled: true
    gateway:
      name: ""  # Defaults to release-name gateway
      namespace: ""

  # -- Model routing list (generates litellm config.yaml model_list)
  # Each entry maps a user-facing model name to a KServe InferenceService endpoint.
  # KServe huggingfaceserver exposes OpenAI-compatible API under /openai/v1.
  #
  # URL pattern: http://<isvc-name>-predictor.<namespace>.svc.cluster.local/openai/v1
  # Provider:    openai/<model-name>  (tells LiteLLM to use OpenAI-compatible protocol)
  #
  # Examples:
  #   # vLLM GPU model (deployed by inference-stack model-serving)
  #   - modelName: qwen25-1-5b
  #     litellmParams:
  #       model: openai/qwen25-1-5b
  #       apiBase: http://qwen25-1-5b-predictor.default.svc.cluster.local/openai/v1
  #       apiKey: no-key-needed  # Required by OpenAI provider, KServe has no auth
  #
  #   # Multi-GPU model with rate limiting
  #   - modelName: llama-70b
  #     litellmParams:
  #       model: openai/llama-70b
  #       apiBase: http://llama-70b-predictor.default.svc.cluster.local/openai/v1
  #       apiKey: no-key-needed
  #       rpm: 60
  #       tpm: 100000
  #
  #   # Route through Inference Gateway for KV-cache aware load balancing
  #   - modelName: llama-70b
  #     litellmParams:
  #       model: openai/llama-70b
  #       apiBase: http://inference-gateway-epp.default.svc.cluster.local
  #       apiKey: no-key-needed
  litellm:
    modelList: []
